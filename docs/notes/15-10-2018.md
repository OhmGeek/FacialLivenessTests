# 15th October 2018
## Notes from Face Flashing source
```
Face Flashing: a Secure Liveness Detection Protocol
based on Light Reflections - https://arxiv.org/pdf/1801.01949.pdf
```

Typical face autghentication system:
- Sensors feed to liveness detection module to the recognition module. These also feed backwards.

Two different types of attacks:
- Static attacks: use of static objects such as photographs, plastic masks etc, and transformation of these objects.
- Dynamic attacks: use dynamic objects.

There are 4 subcategories:
- two dimensional static,
- three dimensional static
- two dimensional dynamic
- three dimensional dynamic

3D branches use stereo objects (e.g. sculptures, silicone masks, and robots).

First method (tracking eye movement) was proprosed by Jee et al (source 9 from the paper). This detects eyes in a sequence of facial images. Blinking is assumed to be natural in this source. 

Lip movement and reading methods were developed by Kollreider et al. These can be fooled using carefully transformed photographs.

Jukka et al observed that printed photographs are pixelated, and therefore faces have more detail (which can be detected using a support vector machine). However, these methods aren't htat robust from motion blur, and are affected by screen-based attacks.

Adversaries have tried using 3d objects in 3d static attacks too, but the method proposed by Lagorio et al to detect facial features is something that can be done. This used two cameras to ascertain the surface curvature of the subject seeking authentication. If the surface curvature is low, the subject is judged to be malicious. While the accuracy is almost 100%, the sophisticated device is expensive and compuational cost is high. While a one camera method was proprosed by Wang et al, this proved problematic with those who wore glasses.

Recently, adversaries have proposed new 2D dynamic attacks which use deep learning to merge a victim's facial characteristics with photos of the victim, and using faked
photographs to bypass the algorithm. While this requires time, photos can be prepared ahead of time, and then offline attacks are used. Some solutions have been proposed.
Bao et al introduced an optical flow based method, which had two drawbacks: uneven objects will fail, and it is not illumination invariant. Kollreider et al developed a method using positions and velocities of facial components usiong Gabor feature classification. But this however ddn't work with significant edges (e.g. beard, glasses). It was shwon that this only works on horizontal rather than vertical motion. Findling et al used a method combining images and sensor data , capturing multiple views of a face. This method, and other similar methods were broken by constructing virtual models of victims based on public photographs.

3D dynamic attacks aren't that common currently. However, these might become more common in future years. They involve construcitng a 3D model of the victim's face, in virtual settings. These are difficult (as a 3D screen is needed, or a sufficiently high quality android is necessary). As this is costly, and involves several hurdles to achieve, the paper discards these methods.

Most attacks tend to be 2D dynamic attacks.

There are 8 steps in the protocol:
#### Step 1: Gneeration of Parameters
We produce parameters such as the seed and N. Seed generates how random challenges are produced, while N determines number of challenges to be used.

#### Step 2: Initialisation of front-end devices
After receiving parameters, the front end device initialises the internal data structure and captures videos.

#### Step 3: Presentation of a challenge
Front end devices being to generate challenges based on the received parameters. A challenge is just a picture displayed on the screen during one refresh period.

Two types of challenges: background challenge - just a pure colour; lighting challenge - lit area over the background colour. 

#### Step 4: Collection of response
The response: the light that's reflected from the subject's fasce. We collect this through the activated camera.

#### Step 5: Repetition of challenge response.
We repeat 3 and 4 a total of N times. This collects enough responses to ensure high security and robustness. 

#### Step 6: Timing Verification
Lighting responses should be immediate - as the speed of light is assumed to be so fast we can't see it in the camera. In counterfeit methods, the response would be presented sequentially (pixel by pixel/row by row).

#### STep 7: Face verification
We use a neural network to verify that the legality of the face is true. If this step isn't present, we can be attacked using MFF attacks.

This makes it difficult to forge.

#### Step 8: Expression Verification
The ability to make expressions indicates liveness. 


### Techniques
#### Model of Light Reflection
Incoming light proportional and simultaneous.

#### Facial Extraction
Locate face. Then extract it. 

We need first to TRACK the face. Then from there, we use facial landmarks.

#### Timing Verification
We need a sense of time based on teh camera and screen.

We assume line by line additions to the screen and captured column by column. While we might change colour, the frame itself might only show top being colour one while the other colour is still the colour from before. When camera refresh is applied too, we expect a different colour.

With the lighting challenge, we have a small band with a different colour to the background.


#### Face Verification
After processing, we get a sequence of frames. We generate a midterm result from the background responses. 

First, choose a random pixel on the face as an anchor point. Divide all pixels by the value of that anchor point.

Then we follow the following steps:

1. Abstract: vidie the face into 20 regions. In each region, we furhter reduce the image into a vector by taking the average value. Next smooth each vector
by applying polynomial fitting of 20 degrees with min 2-norm deviation.

2. Resize. Pick out facial region, resize to a 20 by 20 image by bicubic interpolation.

3. Verify. Feed the resized image into a neural network (3 convolution layers, with a pyramid structure, proved with Cifar-10 dataset).


### Problems:
Using ultra high speed camera might cause problems BUT: this isn't the case with the web directly (and we could also put a cap on the frame limit.)

3D printing is possible, but again light follows a curve so this is more difficult and we could recognise attacks through this.

Also, no implementation exists (YET).

## Image Quality Assessment for Fake Biometric Detection: Application to Iris, Fingerprint, and Face Recognition
[Source](https://ieeexplore.ieee.org/document/6671991/figures#figures)
25 general image quality measures to be used.

Input: biometric sample to classify as fake/real.

Process has no face extraction (designed to work on entire image). therefore quicker.

Method considers LDA and QDA classifiers.

Four  crietereion have been used to pick methods: performance, complementarity (sharpness/entropy), complexity, speed.

### Full Reference IQ Methods
Rely on the reference image to estimate quality of test sample. WE only have access to input sample. Therefore:

Gaussian blur on input greyscale image (sigma 0.5, size 3x3). This generates a smoothed version of I.

Quality computed based on full reference IQA metric.

We use all the calculations for 18 of these metrics from the example table in the source. These involve I, the input image, and I hat (the smoothed image).
These can be calculated using OpenCV in most cases.

### No Reference IQ Measures
These don't require a sample. 

#### Distortion specific approaches
JPEG Quality Index - quality affected by block artifacts
High Low Freq Index - sensitive to sharpness.

#### Traning based approaches
Blind image quality index.
natural image quality evaluator.


### Training the metric
We use these terms as a vector input to a classifier, and then classify whether the output is real or fake based on training data.

## Liveness Detection for Embedded Face Recognition System (source 9 from liveness test)
One of the first liveness tests.
### Proposed Modules:
The following steps are required within the proposed method.

- Input
- Eye Detection
- Face Region Normalisation
- Eye region binarization
- Variation calculation
- Variation < threshold implies a live face. Otherwise fake.

#### Eye Detection
Accurate eye detection is needed.

First, gaussian filtering is performed to smooth the image - this way, the 3D curve of the face is also smoothed.

Then, gradient descent is used to find the local minimums .

An eye classifier is used (trained by AdaBoost) to help reduce invalid eyes.

#### Face Region Normalisation
AS the face can vary in size/orientation, normalise the face about a size and rotation.

We use the center points of both eyes.

With distance of both eyes being d, extract a face candidate from 1/2d outside eyes, 1/2d to upside of eyes, and 3/2d to downside of eyes.
Then normalise face region to 72x72.

From here, apply SQI to face region. This decreases illumination effects.

#### Eye Region Binarization
After normalisation of face region, extract eye regions.

10x20 size based on the centre of eyes.

Binarised eye regions by thresholding.

#### Liveness score calibration
hamming distance used to create liveness score of each eye regon.

Compare two ordered lists of pixels. Compare 5 left eyes with one another using hamming distance and 5 right eyes.
Number of pixels which differ between two regions become liveness score.Then average score.

We use thresholds. 

### Effectiveness
When the source was written, there was no dataset that we could use. However, now there are plenty. As such, this is something we could assess.